

```{r}
#| cache: true
#| cache-extra: !expr rlang::hash(params)
#| cache-file-1: !expr rlang::hash_file("_baseline_query_data.R")
#| cache-file-2: !expr rlang::hash_file("_utils.R")
```


```{r _data_changes_setup_script}
source("_baseline_query_data.R")
```

# Changed Values by Year

```{r changed_values_processing}
# Get only the common columns between runs
common_cols <- intersect(names(comp_chars), names(baseline_chars))

# Subset both datasets to those common columns
comp_common <- comp_chars[, common_cols, drop = FALSE]
baseline_common <- baseline_chars[, common_cols, drop = FALSE]

# Keep only PIN Card combos with data in both runs
card_data <- bind_rows(comp_common, baseline_common) %>%
  group_by(meta_pin, meta_card_num) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(meta_pin, meta_card_num) %>%
  as.data.table()

setnames(card_data, "meta_year", "year")

key_cols <- c("meta_pin", "meta_card_num")
cols <- setdiff(names(card_data), key_cols)

# tolerance list:
# Many of our features should be exact matches but are not. For example,
# our distance to features have some minor variation due to geocoding variance.
# Because of this, all distance features have a buffer of 5 feet.
# There are also features which should not match, but we want to make sure that
# they stay relatively stable. For example, the percentage of college educated
# individuals should not change by more than 10% year over year.

# Create tolerance list
base_tol <- c(
  acs5_median_age_total = 10,
  acs5_median_household_renter_occupied_gross_rent = 400,
  acs5_median_household_total_occupied_year_built = 5,
  acs5_median_income_household_past_year = 10000,
  acs5_median_income_per_capita_past_year = 7500,
  loc_latitude = 0.0001,
  loc_longitude = 0.0001,
  other_tax_bill_rate = 5,
  prox_num_pin_in_half_mile = 10,
  prox_airport_dnl_total = 1,
  prox_num_foreclosure_per_1000_pin_past_5_years = 5,
  prox_num_bus_stop_in_half_mile = 2,
  time_sale_day = 31,
  time_sale_day_of_week = 7,
  time_sale_year = 1,
  year = 1,
  prox_nearest_park_dist_ft = 100,
  prox_nearest_vacant_land_dist_ft = 100,
  meta_sale_count_past_n_years = 1
)

make_tol <- function(cols) {
  tibble(col = cols) %>%
    mutate(val = case_when(
      col %in% names(base_tol) ~ base_tol[match(col, names(base_tol))],
      # Create standard ditstance and acs tolerances
      str_detect(col, "dist_ft$") ~ 5,
      str_detect(col, "^acs5_percent_") ~ 0.1,
      TRUE ~ NA_real_
    )) %>%
    {
      setNames(.$val, .$col)
    }
}

tol <- make_tol(cols)

tol_defined <- !is.na(tol) # TRUE only where a tolerance exists

# Group-wise checks: numeric path ONLY if tolerance is defined
# Otherwise use exact matching
group_matches <- card_data[
  ,
  as.list(mapply(
    function(value, name) {
      t <- tol[[name]]

      # Both NA returns match
      if (all(is.na(value))) {
        return(TRUE)
      }

      # Any single NA present returns no match
      if (any(is.na(value))) {
        return(FALSE)
      }

      # Numeric path if tolerance is defined
      if (isTRUE(tol_defined[[name]])) {
        numeric_value <- suppressWarnings(as.numeric(value))
        (max(numeric_value) - min(numeric_value)) <= t
      } else {
        uniqueN(value) <= 1 # exact match path
      }
    },
    .SD, names(.SD),
    SIMPLIFY = FALSE
  )),
  by = key_cols,
  .SDcols = cols
]


card_matches_long <- melt(
  as.data.table(group_matches),
  id.vars = key_cols,
  variable.name = "column",
  value.name = "is_match"
)

card_matches_long <- card_matches_long %>%
  mutate(tolerance = unname(tol[as.character(column)]))

# Summarize non-matching counts and include tolerance
count_not_matching <- card_matches_long %>%
  filter(!is_match) %>%
  group_by(Column = column) %>%
  summarise(Count = n(), .groups = "drop") %>%
  mutate(Tolerance = unname(tol[as.character(Column)])) %>%
  arrange(desc(Count))
```

## Count of Unmatched Values

```{r unmatched_values}
DT::datatable(
  count_not_matching,
  options = list(
    scrollY = "300px",
    scrollX = TRUE,
    paging = TRUE,
    pageLength = 100,
    searching = TRUE
  ),
  rownames = FALSE
)
```


```{r card_level_changes}
unmatched_values_long <-
  as.data.table(card_data)[
    , melt(
      .SD,
      id.vars = c(key_cols, "year"),
      measure.vars = cols,
      variable.name = "column",
      value.name = "value"
    )
  ][
    # inner join to keep only failed triples
    card_matches_long[is_match == FALSE, .(meta_pin, meta_card_num, column)][
      , column := as.character(column)
    ],
    on = .(meta_pin, meta_card_num, column),
    nomatch = 0
  ][
    order(column, meta_pin, meta_card_num, year)
  ]

make_card_comp_chart <- function(col_name) {
  chart_data_long <- unmatched_values_long[column == col_name]

  # Identify comparison (lower) and base (higher) years
  yrs <- sort(unique(chart_data_long$year))
  y_comp <- yrs[1] # lower year
  y_base <- yrs[2] # higher year

  # Tag rows as new/old and pivot
  chart_data_long[, tag := fcase(
    year == y_comp, "old",
    year == y_base, "new"
  )]

  chart_data_wide <- dcast(
    chart_data_long,
    meta_pin + meta_card_num ~ tag,
    value.var = "value"
  )

  # Rename columns to <col_name>_new and <col_name>_old
  setnames(
    chart_data_wide,
    old = c("new", "old"),
    new = c(paste0(col_name, "_new"), paste0(col_name, "_old"))
  )

  # Cap at 1,000 rows for readability with sample
  # to make sure it's not lower number PINs
  if (nrow(chart_data_wide) > 1000L) {
    chart_data_wide <- chart_data_wide[sample(.N, 1000L)]
  }

  chart_data_wide[]
}

unmatched_tables <- unique(unmatched_values_long$column) %>%
  lapply(make_card_comp_chart) %>%
  (\(x) setNames(x, unique(unmatched_values_long$column)))() %>%
  (\(x) x[!vapply(x, is.null, logical(1))])()

unmatched_charts <- list()

vars <- sort(names(unmatched_tables))

for (var in vars) {
  tbl <- unmatched_tables[[var]]

  unmatched_charts[[var]] <- knitr::kable(
    tbl,
    format = "html",
    caption = sprintf("Unmatched values for %s", var),
    row.names = FALSE,
    align = rep("l", ncol(tbl))
  ) %>%
    kableExtra::kable_styling(
      bootstrap_options = c("striped", "hover", "condensed", "responsive"),
      full_width = FALSE
    ) %>%
    kableExtra::scroll_box(
      height = "400px",
      width = "100%",
      box_css = "border: 1px solid #ddd; padding: 5px;"
    )
}
```

## Unmatched Values by Variable

::: {.panel-tabset}

```{r, results = 'asis'}
for (i in seq_along(unmatched_charts)) {
  cat("### ", names(unmatched_charts)[i], "\n\n")
  print(unmatched_charts[[i]])
  cat("\n\n")
}
```
:::

## Count of Empty Strings (Should be 0)

```{r empty_string_counts}
card_data %>%
  summarize(across(everything(), ~ sum(. == "", na.rm = TRUE))) %>%
  pivot_longer(
    cols = everything(),
    names_to = "column",
    values_to = "empty_string_count"
  ) %>%
  filter(empty_string_count > 0) %>%
  arrange(desc(empty_string_count)) %>%
  DT::datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = TRUE,
      pageLength = 100,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

```{r data_changes_cleanup}
#| output: false

rm(
  card_data, common_cols, comp_common, baseline_common,
  group_matches, card_matches_long, count_not_matching,
  unmatched_values_long, unmatched_tables, unmatched_charts
)
gc()
```
