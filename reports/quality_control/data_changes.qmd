

```{r}
#| cache: true
#| cache-extra: !expr rlang::hash(params)
#| cache-file-1: !expr rlang::hash_file("_baseline_query_data.R")
#| cache-file-2: !expr rlang::hash_file("_utils.R")
```


```{r _data_changes_setup_script}
source("_baseline_query_data.R")
```

# Changed Values by Year

```{r changed_values_processing}
# Get only the common columns between runs
common_cols <- intersect(names(comp_chars), names(baseline_chars))

# Subset both datasets to those common columns
comp_common <- comp_chars[, common_cols, drop = FALSE]
baseline_common <- baseline_chars[, common_cols, drop = FALSE]

# Keep only observations with data in both runs
card_data <- bind_rows(comp_common, baseline_common) %>%
  group_by(meta_pin, meta_card_num) %>%
  filter(n() > 1) %>%
  ungroup() %>%
  arrange(meta_pin, meta_card_num) %>%
  as.data.table()

card_data <- card_data %>%
  slice(1:1000)

setnames(card_data, "meta_year", "year")

key_cols <- c("meta_pin", "meta_card_num")
cols <- setdiff(names(card_data), key_cols)

# tolerance list:
# Many of our features should be exact matches but are not. For example,
# our distance to features have some minor variation due to geocoding variance.
# Because of this, all distance features have a buffer of 5 feet.
# There are also features which should not match, but we want to make sure that
# they stay relatively stable. For example, the percentage of college educated
# individuals should not change by more than 10% year over year.

# Create tolerance list
base_tol <- c(
  acs5_median_age_total = 10,
  acs5_median_household_renter_occupied_gross_rent = 400,
  acs5_median_household_total_occupied_year_built = 5,
  acs5_median_income_household_past_year = 10000,
  acs5_median_income_per_capita_past_year = 7500,
  ccao_n_years_exe_homeowner = 1,
  loc_latitude = 0.0001,
  loc_longitude = 0.0001,
  other_tax_bill_rate = 5,
  prox_num_pin_in_half_mile = 10,
  prox_airport_dnl_total = 1,
  prox_num_foreclosure_per_1000_pin_past_5_years = 5,
  prox_num_bus_stop_in_half_mile = 2,
  time_sale_day = 31,
  time_sale_day_of_week = 7,
  time_sale_year = 1,
  year = 1,
  prox_nearest_park_dist_ft = 100,
  prox_nearest_vacant_land_dist_ft = 100,
  meta_sale_count_past_n_years = 1
)

make_tol <- function(cols) {
  tibble(col = cols) %>%
    mutate(val = case_when(
      col %in% names(base_tol) ~ base_tol[match(col, names(base_tol))],
      str_detect(col, "dist_ft$") ~ 5,
      str_detect(col, "^acs5_percent_") ~ 0.1,
      TRUE ~ NA_real_
    )) %>%
    {
      setNames(.$val, .$col)
    }
}

tol <- make_tol(cols)

tol_defined <- !is.na(tol) # TRUE only where a tolerance exists

# Group-wise checks: numeric path ONLY if tolerance is defined
group_matches <- card_data[
  ,
  as.list(mapply(
    function(v, nm) {
      t <- tol[[nm]]

      # If both values are NA -> match
      if (all(is.na(v))) {
        return(TRUE)
      }

      # Numeric path if tolerance is defined
      if (tol_defined[[nm]]) {
        nv <- suppressWarnings(as.numeric(v))
        if (anyNA(nv)) {
          # If any non-NA mismatch exists, return FALSE
          return(FALSE)
        } else {
          r <- range(nv)
          return((r[2] - r[1]) <= t)
        }
      } else {
        # Exact raw string match
        # If one value is NA (but not both), it's a mismatch
        if (anyNA(v)) {
          return(FALSE)
        }
        # Check all raw values are identical
        return(uniqueN(v) <= 1)
      }
    },
    .SD, names(.SD),
    SIMPLIFY = FALSE
  )),
  by = key_cols,
  .SDcols = cols
]

gm_long <- melt(
  as.data.table(group_matches),
  id.vars = key_cols,
  variable.name = "column",
  value.name = "is_match"
)

# Count of NOT matching values
count_not_matching <- gm_long[is_match == FALSE,
  .(Count = .N),
  by = .(Column = column)
][order(-Count)]
```


## Count of Unmatched Values
```{r unmatched_values}
DT::datatable(
  count_not_matching,
  options = list(
    scrollY = "300px",
    scrollX = TRUE,
    paging = TRUE,
    pageLength = 100,
    searching = TRUE
  ),
  rownames = FALSE
)
```


```{r card_level_changes}
card_data_long <- melt(
  as.data.table(card_data),
  id.vars = c(key_cols, "year"),
  measure.vars = cols,
  variable.name = "column",
  value.name = "value"
)
card_data_long[, `:=`(
  column = as.character(column),
  year   = as.integer(year)
)]

unmatched_idx <- gm_long[is_match == FALSE, .(meta_pin, meta_card_num, column)]
unmatched_idx[, column := as.character(column)]

unmatched_values_long <- merge(
  card_data_long, unmatched_idx,
  by = c("meta_pin", "meta_card_num", "column"),
  all = FALSE
)

setorder(unmatched_values_long, column, meta_pin, meta_card_num, year)
make_col_table_two_years <- function(col_name) {
  sub_long <- unmatched_values_long[column == col_name]
  if (nrow(sub_long) == 0L) {
    return(NULL)
  }

  # Collapse duplicate values per pin/card/year
  sub_long <- sub_long[
    , .(value = paste(unique(na.omit(as.character(value))),
      collapse = " | "
    )),
    by = .(meta_pin, meta_card_num, year)
  ]

  # Identify comparison (lower) and base (higher) years
  yrs <- sort(unique(sub_long$year))
  y_comp <- yrs[1] # lower year
  y_base <- yrs[2] # higher year

  # Tag rows as comp/base and pivot
  sub_long[, tag := fcase(
    year == y_comp, "comp",
    year == y_base, "base"
  )]

  sub_wide <- dcast(
    sub_long,
    meta_pin + meta_card_num ~ tag,
    value.var = "value"
  )

  # Rename columns to <col_name>_base and <col_name>_comp
  setnames(
    sub_wide,
    old = c("base", "comp"),
    new = c(paste0(col_name, "_base"), paste0(col_name, "_comp"))
  )

  # Order columns
  setcolorder(
    sub_wide,
    c(
      "meta_pin", "meta_card_num",
      paste0(col_name, "_base"), paste0(col_name, "_comp")
    )
  )

  # Cap at 1,000 rows for readability with sample
  # to make sure it's not lower number PINs
  if (nrow(sub_wide) > 1000L) sub_wide <- sub_wide[sample(.N, 1000L)]

  sub_wide[]
}

vars_with_unmatched <- unique(unmatched_values_long$column)
unmatched_tables <- lapply(vars_with_unmatched, make_col_table_two_years)
names(unmatched_tables) <- vars_with_unmatched
unmatched_tables <- unmatched_tables[!vapply(
  unmatched_tables,
  is.null, logical(1)
)]
```


```{r}
unmatched_charts <- list()
vars <- names(unmatched_tables)
prefix <- stringr::str_extract(vars, "^[^_]+")
vars_sorted <- vars[order(prefix, vars)]

unmatched_charts <- list()

for (var in vars_sorted) {
  tbl <- unmatched_tables[[var]]

  unmatched_charts[[var]] <- knitr::kable(
    tbl,
    format = "pipe",
    caption = sprintf("Unmatched values for %s", var),
    row.names = FALSE,
    align = rep("l", ncol(tbl))
  )
}
```
## Unmatched Values by Variable

::: {.panel-tabset}

```{r, results = 'asis'}
for (i in seq_along(unmatched_charts)) {
  cat("### ", names(unmatched_charts)[i], "\n\n")
  print(unmatched_charts[[i]])
  cat("\n\n")
}
```
:::

## Count of Empty Strings (Should be 0)

```{r empty_string_counts}
card_data %>%
  summarize(across(everything(), ~ sum(. == "", na.rm = TRUE))) %>%
  pivot_longer(
    cols = everything(),
    names_to = "column",
    values_to = "empty_string_count"
  ) %>%
  filter(empty_string_count > 0) %>%
  arrange(desc(empty_string_count)) %>%
  DT::datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = TRUE,
      pageLength = 100,
      searching = TRUE
    ),
    rownames = FALSE
  )
```
