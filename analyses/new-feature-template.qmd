---
title: "New Feature Template"
subtitle: "Run ID: `r params$added_feature`: `r params$description`"
date: "`r Sys.Date()`"
author: "Cook County Assessor's Office Data Department"
execute:
  echo: false
  warning: false
  asis: true
format:
  html:
    embed-resources: true
    toc: true
    toc_float: true
    fig-align: center
    fontsize: 12pt
params:
  run_id: "2025-02-11-charming-eric"
  run_id_year: "2025"
  comparison_run_id: "2025-01-10-serene-boni"
  comparison_run_id_year: "2025"
  added_feature: "time_sale_roll_mean_nbhd_t0_w3"
  added_feature_shap: "time_sale_roll_mean_nbhd_t0_w3_shap"
  description: "Added feature to calculate neighborhood rolling averages of sale price"
  min_range: 5
  max_range: 95
  type: "continuous"
---

```{r packages, include = FALSE}
library(purrr)
library(here)
library(yaml)
library(corrplot)
# Load list of helper files and main libraries
purrr::walk(list.files(here::here("R"), "\\.R$", full.names = TRUE), source)

# Load reporting-only R libraries
suppressPackageStartupMessages({
  reporting_libs <- "Config/renv/profiles/reporting/dependencies"
  purrr::walk(
    strsplit(read_yaml(here::here("DESCRIPTION"))[[reporting_libs]], ", ")[[1]],
    library,
    character.only = TRUE
  )
})

# TODO: Catch for weird Arrow bug with SIGPIPE. Need to permanently fix later
# https://github.com/apache/arrow/issues/32026
cpp11::cpp_source(code = "
#include <csignal>
#include <cpp11.hpp>

[[cpp11::register]] void ignore_sigpipes() {
  signal(SIGPIPE, SIG_IGN);
}
")

ignore_sigpipes()
```

```{r download_new_data, include = FALSE}
base_paths <- model_file_dict(params$run_id, params$run_id_year)
comparison_paths <- model_file_dict(
  params$comparison_run_id,
  params$comparison_run_id_year
)
run_id <- params$run_id
comparison_run_id <- params$comparison_run_id

analyses_paths <- list(
  output = list(
    list(
      s3 = base_paths$output$assessment_card$s3,
      key = "assessment_card"
    ),
    list(
      s3 = base_paths$output$assessment_pin$s3,
      key = "assessment_pin"
    ),
    list(
      s3 = base_paths$output$metadata$s3,
      key = "metadata"
    ),
    list(
      s3 = base_paths$output$performance_test$s3,
      key = "performance_test"
    ),
    list(
      s3 = base_paths$output$shap$s3,
      key = "shap"
    )
  )
)

source("helpers.R")
data_new <- model_fetch_run_subset(
  params$run_id,
  params$run_id_year,
  analyses_paths, TRUE
)

list2env(data_new, envir = .GlobalEnv)

rm(data_new)

comparison_paths <- list(
  output = list(
    list(
      s3 = comparison_paths$output$assessment_pin$s3,
      key = "assessment_pin"
    ),
    list(
      s3 = comparison_paths$output$performance_test$s3,
      key = "performance_test"
    )
  )
)

data_comparison <- model_fetch_run_subset(
  params$comparison_run_id,
  params$comparison_run_id_year,
  comparison_paths, TRUE
)

list2env(data_comparison, envir = .GlobalEnv)

rm(data_comparison)

all_vars <- ls()

# Iterate over all variables and rename if necessary
for (var_name in all_vars) {
  rename_var(var_name, params$run_id, "_new")
  rename_var(var_name, params$comparison_run_id, "_comparison")
}

lockfile_assessment <- metadata_new$dvc_md5_assessment_data

s3_path_assessment <- paste0(
  "s3://ccao-data-dvc-us-east-1/files/md5/",
  substr(lockfile_assessment, 1, 2), "/",
  substr(lockfile_assessment, 3, nchar(lockfile_assessment))
)

assessment_data_new <- read_parquet(s3_path_assessment)
```


```{r, data_transformation}
target_feature_value <- params$added_feature
target_feature_shap <- params$added_feature_shap
type <- params$type
nbhd <- ccao::nbhd_shp

# Create a individual card level dataset
card_individual <- shap_new %>%
  select(
    meta_pin, meta_card_num, pred_card_shap_baseline_fmv,
    {{ target_feature_value }}
  ) %>%
  rename(!!sym(target_feature_shap) := !!sym(target_feature_value)) %>%
  inner_join(
    assessment_card_new %>%
      select(
        meta_pin, meta_nbhd_code,
        meta_card_num,
        pred_card_initial_fmv,
        {{ target_feature_value }}
      ),
    by = c("meta_pin", "meta_card_num")
  )

# Summarizing data by neighborhood code
card_nbhd <- card_individual %>%
  group_by(meta_nbhd_code) %>%
  summarize(
    !!paste0({{ target_feature_shap }}, "_mean") :=
      mean(!!sym({{ target_feature_shap }}), na.rm = TRUE),
    !!paste0({{ target_feature_shap }}, "_90th") :=
      quantile(!!sym({{ target_feature_shap }}), probs = 0.9, na.rm = TRUE),
    !!paste0({{ target_feature_shap }}, "_mean_abs") :=
      mean(abs(!!sym({{ target_feature_shap }})), na.rm = TRUE)
  ) %>%
  ungroup() %>%
  inner_join(
    nbhd,
    by = c("meta_nbhd_code" = "town_nbhd")
  ) %>%
  st_as_sf()

## Create a pin level dataset
pin_individual <- assessment_pin_new %>%
  select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv) %>%
  rename(
    pred_pin_final_fmv_new = pred_pin_final_fmv,
    pred_pin_initial_fmv_new = pred_pin_initial_fmv
  ) %>%
  inner_join(
    assessment_pin_comparison %>%
      select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv),
    by = "meta_pin"
  ) %>%
  rename(
    pred_pin_final_fmv_comp = pred_pin_final_fmv,
    pred_pin_initial_fmv_comp = pred_pin_initial_fmv
  ) %>%
  mutate(
    diff_pred_pin_final_fmv = round((
      (pred_pin_final_fmv_new - pred_pin_final_fmv_comp) /
        pred_pin_final_fmv_comp
    ), 4),
    pred_pin_final_fmv_new = dollar(pred_pin_final_fmv_new),
    pred_pin_final_fmv_comp = dollar(pred_pin_final_fmv_comp),
    diff_pred_pin_initial_fmv = round((
      (pred_pin_initial_fmv_new - pred_pin_initial_fmv_comp) /
        pred_pin_initial_fmv_comp
    ), 4),
    pred_pin_initial_fmv_new = dollar(pred_pin_initial_fmv_new),
    pred_pin_initial_fmv_comp = dollar(pred_pin_initial_fmv_comp)
  ) %>%
  inner_join(
    assessment_data_new %>%
      distinct(meta_pin, .keep_all = TRUE) %>%
      select(
        meta_pin, meta_nbhd_code, loc_longitude,
        loc_latitude, meta_township_name, {{ target_feature_value }}
      ),
    by = "meta_pin"
  )

# Aggregate to neighborhood level
if (type == "continuous") {
  pin_nbhd <- pin_individual %>%
    group_by(meta_nbhd_code) %>%
    summarize(
      !!paste0(target_feature_value, "_neighborhood_mean") :=
        mean(!!sym(target_feature_value), na.rm = TRUE),
      !!paste0(target_feature_value, "_neighborhood_median") :=
        median(!!sym(target_feature_value), na.rm = TRUE),
      !!paste0(target_feature_value, "_neighborhood_90th") :=
        quantile(!!sym(target_feature_value), 0.9, na.rm = TRUE)
    ) %>%
    inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
    st_as_sf()
} else {
  pin_nbhd <- pin_individual %>%
    group_by(meta_nbhd_code, !!sym(target_feature_value)) %>%
    count() %>%
    ungroup() %>%
    group_by(meta_nbhd_code) %>%
    mutate(
      percentage = n / sum(n) * 100
    ) %>%
    arrange(meta_nbhd_code, desc(n)) %>%
    mutate(
      plurality_factor = first(!!sym(target_feature_value))
    ) %>%
    ungroup() %>%
    select(
      meta_nbhd_code,
      !!sym(target_feature_value),
      percentage,
      plurality_factor
    ) %>%
    pivot_wider(
      names_from = !!sym(target_feature_value),
      values_from = percentage,
      names_prefix = "percentage_"
    ) %>%
    inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
    st_as_sf()
}

# Pivot wider for leaflet maps to allow multiple shap values
leaflet_data <- card_individual %>%
  select(meta_pin, {{ target_feature_shap }}) %>%
  group_by(meta_pin) %>%
  mutate(
    shap_total = sum(!!sym({{ target_feature_shap }})),
    variable_index = row_number(),
    name_col = paste0(
      deparse(substitute(
        target_feature_shap
      )), "_",
      variable_index
    )
  ) %>%
  pivot_wider(
    id_cols = c("meta_pin", "shap_total"),
    names_from = name_col,
    values_from = !!sym({{ target_feature_shap }})
  ) %>%
  ungroup() %>%
  right_join(pin_individual, by = c("meta_pin" = "meta_pin")) %>%
  mutate(
    pred_pin_initial_fmv_new_numeric =
      as.numeric(gsub("[$,]", "", pred_pin_initial_fmv_new)),
    relative_shap =
      round(as.numeric(shap_total) / pred_pin_initial_fmv_new_numeric, 2)
  ) %>%
  distinct(meta_pin, .keep_all = TRUE)
```


# Descriptive Statistics

::: {.panel-tabset}

```{r}
create_summary_table <- function(data, target_feature, group_by_column = NULL) {
  target_feature <- sym(target_feature)

  if (!is.null(group_by_column)) {
    formatted_group_by_column <- str_to_title(
      str_replace_all(group_by_column, "_", " ")
    )

    summary_data <- data %>%
      group_by(!!sym(group_by_column)) %>%
      summarize(
        !!formatted_group_by_column := first(!!sym(group_by_column)),
        Mean = round(mean(!!target_feature, na.rm = TRUE), 2),
        Median = round(median(!!target_feature, na.rm = TRUE), 2),
        `10th Percentile` = round(
          quantile(!!target_feature, 0.1, na.rm = TRUE),
          2
        ),
        `90th Percentile` = round(
          quantile(!!target_feature, 0.9, na.rm = TRUE),
          2
        ),
        Mode = round(
          as.numeric(
            names(sort(table(!!target_feature), decreasing = TRUE)[1])
          ),
          2
        )
      ) %>%
      select(-!!sym(group_by_column))
  } else {
    summary_data <- data %>%
      summarize(
        Mean = round(mean(!!target_feature, na.rm = TRUE), 2),
        Median = round(median(!!target_feature, na.rm = TRUE), 2),
        `10th Percentile` = round(
          quantile(!!target_feature, 0.1, na.rm = TRUE),
          2
        ),
        `90th Percentile` = round(
          quantile(!!target_feature, 0.9, na.rm = TRUE),
          2
        ),
        Mode = round(
          as.numeric(
            names(sort(table(!!target_feature), decreasing = TRUE)[1])
          ),
          2
        )
      )
  }

  # Display the summary in a datatable
  datatable(
    summary_data,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
}
```

## Overall

```{r mean_median}
create_summary_table(
  pin_individual,
  target_feature = {{ target_feature_value }}
)
```

## Township
```{r mean_median_township}
# For Township
create_summary_table(
  pin_individual,
  target_feature = {{ target_feature_value }},
  group_by_column = "meta_township_name"
)
```

## Neighborhood

```{r mean_median_neighborhood}
# For Neighborhood
create_summary_table(
  pin_individual,
  target_feature = {{ target_feature_value }},
  group_by_column = "meta_nbhd_code"
)
```

:::

# Histogram

::: {.panel-tabset}


```{r, histogram_function}
create_histogram_with_statistics <- function(data,
                                             target_feature,
                                             x_label,
                                             y_label = "Frequency",
                                             filter_outliers = FALSE,
                                             filter_column = NULL) {
  # Conditionally filter outliers if requested
  if (filter_outliers && !is.null(filter_column)) {
    data <- data %>%
      filter(
        !!sym(filter_column) >= quantile(
          !!sym(filter_column),
          0.025,
          na.rm = TRUE
        ),
        !!sym(filter_column) <= quantile(
          !!sym(filter_column),
          0.975,
          na.rm = TRUE
        )
      )
  }

  # Calculate mean and median
  data <- data %>%
    mutate(
      mean_value = mean(!!sym(target_feature), na.rm = TRUE),
      median_value = median(!!sym(target_feature), na.rm = TRUE)
    )

  # Create the plot
  plot <- data %>%
    ggplot(aes(x = !!sym(target_feature))) +
    geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
    geom_vline(
      aes(xintercept = mean_value, color = "Mean"),
      linetype = "dashed",
      linewidth = 1,
      show.legend = TRUE
    ) +
    geom_vline(
      aes(xintercept = median_value, color = "Median"),
      linetype = "dashed",
      linewidth = 1,
      show.legend = TRUE
    ) +
    scale_color_manual(
      name = "Statistics",
      values = c(Mean = "red", Median = "green"),
      labels = c("Mean", "Median")
    ) +
    labs(
      x = x_label,
      y = y_label
    ) +
    theme_minimal()

  return(plot)
}
```

## Feature Histogram
```{r, feature_histogram}
create_histogram_with_statistics(
  data = pin_individual,
  target_feature = target_feature_value,
  x_label = target_feature_value
)
```

## FMV Change Histogram

This chart shows the distribution of the difference between 'pred_pin_initial_fmv' in the model with the added feature minus the model without the added feature. Outliers outside of 95% are removed to make the chart more readable.
```{r, fmv_change_histogram}
create_histogram_with_statistics(
  data = pin_individual,
  target_feature = "diff_pred_pin_initial_fmv",
  x_label = "Change in FMV",
  filter_outliers = TRUE,
  filter_column = "diff_pred_pin_initial_fmv"
)
```

## SHAP Histogram

```{r, shap_histogram}
create_histogram_with_statistics(
  data = shap_new,
  target_feature = target_feature_value,
  x_label = "SHAP Value",
  filter_outliers = TRUE,
  filter_column = target_feature_value
)
```

:::

# Correlations

::: panel-tabset

## Correlation Between Added Feature and Other Features

Here, the goal is to see if the added feature *very* neatly aligns with other existing features. Columns are produced with both the absolute value of the correlation (for easy sorting), as well as the raw value to help decipher the direction of the relationship.

```{r correlation_between_features}
columns_to_remove <- c(
  "Sale Year",
  "Sale Month of Year",
  "Sale Day of Year",
  "Sale Day of Week",
  "Sale Day of Month"
)

if (params$type == "continuous") {
  numeric_cols <- assessment_data_new %>%
    rename(
      "Property Class Meta" = meta_class,
      "Property Class Char" = char_class
    ) %>%
    ccao::vars_rename(
      names_from = "model",
      names_to = "pretty",
      output_type = "inplace",
      dictionary = ccao::vars_dict
    ) %>%
    select_if(is.numeric) %>%
    select(-all_of(columns_to_remove))

  # Initialize a data frame to store correlation results
  correlation_results <- data.frame(
    Feature = character(),
    Correlation = numeric(),
    Abs_Correlation = numeric(),
    stringsAsFactors = FALSE
  )

  # Loop through each numeric column and calculate correlation and
  # absolute correlation
  for (col_name in names(numeric_cols)) {
    # Filter out rows with missing values in the two columns being compared
    complete_cases <- complete.cases(
      numeric_cols[[col_name]],
      assessment_data_new[[params$added_feature]]
    )

    # Only compute correlation if there are complete cases
    if (sum(complete_cases) > 0) {
      correlation_value <- cor(
        numeric_cols[[col_name]][complete_cases],
        assessment_data_new[[params$added_feature]][complete_cases],
        use = "complete.obs"
      )
      abs_correlation_value <- abs(cor(
        abs(numeric_cols[[col_name]][complete_cases]),
        abs(assessment_data_new[[params$added_feature]][complete_cases]),
        use = "complete.obs"
      ))
      correlation_results <- rbind(
        correlation_results,
        data.frame(
          Feature = col_name,
          Correlation = correlation_value,
          Abs_Correlation = abs_correlation_value
        )
      )
    }
  }

  # Sort the correlation results in descending order by Correlation
  correlation_results <- correlation_results %>%
    arrange(dplyr::desc(Abs_Correlation)) %>%
    mutate(across(where(is.numeric), ~ round(., 2)))

  top_10_features <- correlation_results %>%
    slice(1:10) %>%
    pull(Feature)

  correlation_results_clean <- correlation_results %>%
    clean_column_values("Feature") %>%
    slice(2:n())


  # Display the correlation results as a scrollable table
  datatable(
    correlation_results_clean,
    options = list(
      scrollX = TRUE,
      scrollY = TRUE,
      pageLength = 10,
      order = list(list(1, "desc"))
    )
  )
} else {
  print(paste(
    "assessment_data_new$",
    params$added_feature,
    " is not numeric.",
    sep = ""
  ))
}
```

## Correlation Plot of 10 Features (absolute value)

```{r top_10_correlation_plot}
# Select the top 10 features, remove rows with NA values, rename columns,
# calculate the correlation, and plot the correlation matrix
assessment_data_new %>%
  rename(
    "Property Class Meta" = meta_class,
    "Property Class Char" = char_class
  ) %>%
  ccao::vars_rename(
    names_from = "model",
    names_to = "pretty",
    output_type = "inplace",
    dictionary = ccao::vars_dict
  ) %>%
  select_if(is.numeric) %>%
  select(-all_of(columns_to_remove)) %>%
  select(all_of(top_10_features)) %>%
  na.omit() %>%
  rename_with(
    ~ gsub("^meta_|^prox_|^other_|^loc_|^char_|^acs5|^acs_|^ccao_", "", .)
  ) %>%
  rename_with(~ gsub("_", " ", .)) %>%
  cor() %>%
  corrplot(
    method = "circle",
    tl.cex = 0.6,
    tl.srt = 45,
    addgrid.col = "grey",
    mar = c(1, 1, 1, 1)
  )
```
:::

# Ratio Stats
```{r, ratio_stats_function}
ratio_stats <- performance_test_new %>%
  filter(
    by_class == FALSE,
    geography_type %in% c("nbhd_code", "township_code", "triad_code")
  ) %>%
  inner_join(
    performance_test_comparison %>%
      filter(
        by_class == FALSE,
        geography_type %in% c("nbhd_code", "township_code", "triad_code")
      ),
    by = c("geography_type", "geography_id", "triad_code"),
    suffix = c("_new", "_comparison")
  ) %>%
  mutate(
    cod_diff = cod_new - cod_comparison,
    prd_diff = prd_new - prd_comparison,
    prb_diff = prd_new - prb_comparison,
    mki_diff = mki_new - mki_comparison,
    median_ratio_diff = median_ratio_new - median_ratio_comparison,
    rmse_diff = rmse_new - rmse_comparison,
    r_squared_diff = r_squared_new - r_squared_comparison
  ) %>%
  mutate(across(where(is.numeric), ~ round(.x, 2))) %>%
  select(
    triad_code,
    geography_type,
    geography_id,
    cod_new,
    cod_comparison,
    cod_diff,
    prd_new,
    prd_comparison,
    prd_diff,
    prd_new,
    prb_comparison,
    prb_diff,
    mki_new,
    mki_comparison,
    mki_diff,
    median_ratio_new,
    median_ratio_comparison,
    median_ratio_diff,
    rmse_new,
    rmse_comparison,
    rmse_diff,
    r_squared_new,
    r_squared_comparison,
    r_squared_diff
  ) %>%
  rename_with(
    ~ str_replace_all(., "_", " ") %>%
      str_to_title() %>%
      str_replace_all(., " ", " ")
  ) %>%
  split(.$"Geography Type")
```

::: {.panel-tabset}

## Triad

```{r, ratio_stats_triad}
ratio_stats[[3]] %>%
  select(-c("Geography Id", "Geography Type")) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

## Township

```{r, ratio_stats_township}
ratio_stats[[2]] %>%
  select(-c("Geography Type")) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

## Neighborhood

```{r, ratio_stats_neighborhood}
ratio_stats[[1]] %>%
  select(-c("Geography Type")) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

:::

# SHAP

The primary metric that the CCAO Data team uses to assess the importance of a feature is its SHAP value. SHAP values provide the amount of value each feature contributes to a parcel's predicted value. The SHAP value is calculated for each observation in the dataset, and the median SHAP value for a feature is used to determine the relative influence of that feature. The higher the median SHAP value, the more influential the feature is in the model.

::: {.panel-tabset}

```{r, shap_count}
shap_predictors <- unlist(metadata_new$model_predictor_all_name)
```

```{r shap_processing}
# Combine data
shap_df_filtered_long <- shap_new %>%
  inner_join(
    assessment_data_new %>%
      select(meta_pin, meta_card_num, meta_township_code, meta_nbhd_code) %>%
      rename(
        township_code = meta_township_code,
        neighborhood_code = meta_nbhd_code
      ),
    by = c("meta_pin", "meta_card_num")
  ) %>%
  select(township_code, all_of(shap_predictors)) %>%
  pivot_longer(
    cols = all_of(shap_predictors),
    names_to = "feature",
    values_to = "shap"
  )
```

### SHAP Median Absolute Value
The following table produces the median absolute SHAP value by township, and creates a grouped table. In total, there are `r length(shap_predictors)` indicators in the model. Thus, if the median SHAP is ranked 1, it is the most important feature, while if it is ranked `r length(shap_predictors)`, it is the least important feature in a township. The median value (without absolute) is also included to better contextualize the impact.

```{r shap_full_importance}
shap_df_filtered_long %>%
  group_by(feature) %>%
  mutate(
    median_abs_shap = round(median(abs(shap), na.rm = TRUE), 2),
    median_shap = round(median(shap, na.rm = TRUE), 2)
  ) %>%
  ungroup() %>%
  distinct(feature, .keep_all = TRUE) %>%
  arrange(desc(median_abs_shap)) %>%
  mutate(
    rank_absolute = row_number(),
    `Median Absolute Shap` = scales::dollar(median_abs_shap),
    `Median SHAP` = scales::dollar(median_shap)
  ) %>%
  inner_join(ccao::town_dict, by = c("township_code" = "township_code")) %>%
  ccao::vars_rename(
    names_from = "model",
    names_to = "pretty",
    output_type = "inplace",
    dictionary = ccao::vars_dict
  ) %>%
  clean_column_values("feature") %>%
  select(
    Feature = feature,
    "Median Absolute Shap",
    "Median SHAP",
    "Rank Absolute" = rank_absolute
  ) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

### SHAP Median Absolute Value by Township

This is the same table, except ranked by township. To identify the rank of an indicator within a township, simply search for that township in the search bar.
```{r shap_township_importance}
shap_df_filtered_long %>%
  group_by(township_code, feature) %>%
  mutate(
    median_abs_shap = round(median(abs(shap), na.rm = TRUE), 2),
    median_shap = round(median(shap, na.rm = TRUE), 2)
  ) %>%
  ungroup() %>%
  distinct(township_code, feature, .keep_all = TRUE) %>%
  group_by(township_code) %>%
  arrange(desc(median_abs_shap), .by_group = TRUE) %>%
  mutate(
    township_rank_absolute = row_number(),
    `Median Absolute Shap` = scales::dollar(median_abs_shap),
    `Median SHAP` = scales::dollar(median_shap)
  ) %>%
  ungroup() %>%
  inner_join(ccao::town_dict, by = c("township_code" = "township_code")) %>%
  ccao::vars_rename(
    names_from = "model",
    names_to = "pretty",
    output_type = "inplace",
    dictionary = ccao::vars_dict
  ) %>%
  clean_column_values("feature") %>%
  select(
    Township = township_name,
    `Township Code` = township_code,
    Feature = feature,
    `Median Absolute Shap`,
    `Median SHAP`,
    `Township Rank Absolute` = township_rank_absolute
  ) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

### Violin Plot Comparing SHAP to Feature
```{r violin_plots_shap_to_feature}
# Calculate the number of digits
num_digits <- card_individual %>%
  pull({{ target_feature_shap }}) %>%
  max(na.rm = TRUE) %>%
  floor() %>%
  as.character() %>%
  str_length()

quantiles <- card_individual %>%
  pull({{ target_feature_shap }}) %>%
  quantile(c(0.025, 0.975), na.rm = TRUE)

# Create the violin plot, excluding outliers only in the display
card_individual %>%
  select(
    meta_card_num, meta_pin,
    {{ target_feature_shap }},
    {{ target_feature_value }}
  ) %>%
  mutate(
    bin = cut_number(!!sym(target_feature_value), n = 10, dig.lab = num_digits)
  ) %>%
  ggplot(aes(x = bin, y = !!sym(target_feature_shap))) +
  geom_violin(fill = "#69b3a2") +
  theme_minimal() +
  xlab("Feature Value") +
  ylab("SHAP Value") +
  scale_x_discrete(labels = function(x) {
    x <- gsub("\\.[^,\\]]*", "", x) # Clean the factor levels for chart
    x <- gsub("[^0-9,,]", "", x)
    gsub(",", "-", x)
  }) +
  # Focus only on the range between the 2.5% and 97.5% quantiles
  coord_cartesian(ylim = quantiles) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

### Scatterplot Demonstrating the Relationship between SHAP and Feature Value

```{r, SHAP_scatterplot}
shapviz::shapviz(
  object = shap_new %>%
    select(all_of(shap_predictors)) %>%
    as.matrix(),
  X = assessment_card_new %>%
    select(all_of(shap_predictors)),
  baseline = shap_new$pred_card_shap_baseline_fmv[1]
) %>%
  shapviz::sv_dependence(
    v = target_feature_value
  )
```
:::

# Spatial Analysis

## Neighborhood Values

::: panel-tabset

### Mean Value of the Feature

```{r mean_feature_neighborhood}
pin_nbhd %>%
  ggplot() +
  geom_sf(aes(
    fill = !!sym(paste0({{ target_feature_value }}, "_neighborhood_mean"))
  )) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Median Value of the Feature

```{r median_feature_neighborhood}
pin_nbhd %>%
  ggplot() +
  geom_sf(aes(
    fill = !!sym(paste0({{ target_feature_value }}, "_neighborhood_median"))
  )) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Mean Absolute SHAP Value

```{r mean_shap_neighborhood}
card_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_shap }}, "_mean_abs")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### 90th Percentile of Absolute SHAP

```{r 90th_percentile_shap}
card_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_shap }}, "_90th")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Median change in Neighborhood FMV
This value is defined as the neighborhood level median increase in value when adding the new feature to the model. For example, a value of 1% would mean that adding the feature increased properties within a neighborhood by 1%.

```{r neighborhood_change}
assessment_pin_new %>%
  select(meta_pin, loc_longitude, loc_latitude, pred_pin_final_fmv) %>%
  rename(pred_pin_final_fmv_new = pred_pin_final_fmv) %>%
  inner_join(assessment_pin_comparison, by = "meta_pin") %>%
  rename(pred_pin_final_fmv_comparison = pred_pin_final_fmv) %>%
  group_by(meta_nbhd_code) %>%
  summarize(
    median_fmv_new = median(pred_pin_final_fmv_new, na.rm = TRUE),
    median_fmv_comparison = median(pred_pin_final_fmv_comparison, na.rm = TRUE),
    fmv_ratio = (median_fmv_new / median_fmv_comparison) / median_fmv_comparison
  ) %>%
  inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
  st_as_sf() %>%
  ggplot() +
  geom_sf(aes(fill = fmv_ratio)) +
  scale_fill_viridis_c(
    option = "viridis",
    name = "FMV Ratio",
    labels = scales::percent_format(accuracy = 0.001)
  ) +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```
:::
# Leaflet Maps

```{r, leaflet_function}
create_leaflet_map <- function(dataset,
                               legend_value,
                               legend_title,
                               order_scheme = "high",
                               longitude = "loc_longitude",
                               latitude = "loc_latitude",
                               display_as_percent = FALSE) {
  # Filter neighborhoods that have at least one observation
  nbhd_borders <- nbhd %>%
    right_join(dataset, by = c("town_nbhd" = "meta_nbhd_code"))

  # Adjust the dataset values if display_as_percent is TRUE
  if (display_as_percent) {
    dataset[[legend_value]] <- dataset[[legend_value]] * 100
  }

  # Create the color palette based on order_scheme
  if (order_scheme == "low") {
    pal <- colorNumeric(
      palette = "Reds",
      domain = dataset[[legend_value]],
      reverse = TRUE
    )
  } else {
    pal <- colorNumeric(palette = "Reds", domain = dataset[[legend_value]])
  }

  # Calculate the bounding box of the filtered neighborhoods
  bbox <- st_bbox(nbhd_borders)

  # Create the leaflet map
  leaflet(dataset) %>%
    addProviderTiles(providers$CartoDB.Positron) %>%
    addCircleMarkers(
      lng = ~ get(longitude),
      lat = ~ get(latitude),
      radius = 5,
      color = ~ pal(dataset[[legend_value]]),
      popup = ~ {
        shap_values <- dataset %>%
          select(starts_with("target_feature_shap_")) %>%
          summarise_all(
            ~ ifelse(!is.na(.), sprintf("SHAP: %s", scales::dollar(.)), NA)
          ) %>%
          apply(1, function(row) {
            paste(na.omit(row), collapse = "<br>")
          })
        paste(
          "Pin: ", meta_pin,
          ifelse(shap_values == "", "", paste0("<br>", shap_values)),
          "<br>", "Relative SHAP: ", scales::percent(
            relative_shap,
            accuracy = 0.01
          ),
          "<br>", "Feature: ", sprintf("%.2f", get(params$added_feature)),
          "<br>", "New FMV: ", pred_pin_final_fmv_new,
          "<br>", "Comparison FMV: ", pred_pin_final_fmv_comp,
          "<br>", "FMV Difference: ", scales::percent(diff_pred_pin_final_fmv)
        )
      }
    ) %>%
    addPolygons(
      data = nbhd_borders,
      color = "black",
      weight = 2,
      fill = FALSE
    ) %>%
    addLegend(
      "bottomright",
      pal = pal,
      values = dataset[[legend_value]],
      title = legend_title,
      labFormat = if (display_as_percent) {
        labelFormat(suffix = "%")
      } else {
        labelFormat()
      }
    )
}
```

## Highest and Lowest 100 Values

::: panel-tabset

### Largest 100 Values

Be careful interpreting values which are the max and min of the raw value, since ties are not accounted for. For example, if there are 10,000 parcels which are 0 feet from a newly constructed building, the map will not be a full representation.

```{r largest_values}
highest_100 <- leaflet_data %>%
  arrange(desc(!!sym(target_feature_value))) %>%
  dplyr::slice(1:100)

create_leaflet_map(
  highest_100,
  {{ target_feature_value }},
  "Largest 100 Values"
)
```

### Lowest 100 Values

Be careful interpreting values which are the max and min of the raw value, since ties are not accounted for. For example, if there are 10,000 parcels which are 0 feet from a newly constructed building, the map will not be a full representation.

```{r, 100_lowest}
lowest_100 <- leaflet_data %>%
  distinct(meta_pin, .keep_all = TRUE) %>%
  arrange(!!sym({{ target_feature_value }})) %>%
  slice(1:100)

create_leaflet_map(
  lowest_100,
  {{ target_feature_value }},
  "Lowest 100 Values",
  order_scheme = "low"
)
```

### Highest 100 SHAP Values

```{r, 100_highest_shap}
highest_100 <- leaflet_data %>%
  arrange(desc(shap_total)) %>%
  slice(1:100)

create_leaflet_map(highest_100, "shap_total", "Highest 100 SHAPs")
```

### Lowest 100 SHAP Values

```{r, 100_lowest_shap}
lowest_100 <- leaflet_data %>%
  arrange(shap_total) %>%
  slice(1:100)

create_leaflet_map(
  lowest_100,
  "shap_total",
  "Lowest 100 SHAPs",
  order_scheme = "low"
)
```
:::

## Largest FMV Changes

Multicard parcels have heuristic which limits their change. The added feature may trigger (or not trigger it), leading to changes much larger than the added feature's impact.

::: panel-tabset

### 100 Largest FMV Increases

```{r, 100_largest_fmv_increases}
largest_fmv_increases <- leaflet_data %>%
  arrange(desc(diff_pred_pin_final_fmv)) %>%
  slice(1:100)

# Call the function with the pre-sliced dataset
create_leaflet_map(
  largest_fmv_increases,
  "diff_pred_pin_final_fmv",
  "Largest FMV Increases",
  display_as_percent = TRUE
)
```

### 100 Largest FMV Decreases

```{r, 100_largest_fmv_decreases}}
largest_fmv_decreases <- leaflet_data %>%
  arrange(diff_pred_pin_final_fmv) %>%
  slice(1:100)

create_leaflet_map(
  largest_fmv_decreases,
  "diff_pred_pin_final_fmv",
  "Largest FMV Decreases",
  order_scheme = "low",
  display_as_percent = TRUE
)
```

### 100 Largest FMV Initial Increases

```{r, 100_largest_fmv_initial_increases}
largest_fmv_increases <- leaflet_data %>%
  arrange(desc(diff_pred_pin_initial_fmv)) %>%
  slice(1:100)

# Call the function with the pre-sliced dataset
create_leaflet_map(
  largest_fmv_increases,
  "diff_pred_pin_initial_fmv",
  "Largest FMV Increases",
  display_as_percent = TRUE
)
```

### 100 Largest Initial FMV Decreases

```{r, 100_largest_fmv_initial_decreases}}
largest_fmv_decreases <- leaflet_data %>%
  arrange(diff_pred_pin_initial_fmv) %>%
  slice(1:100)

create_leaflet_map(
  largest_fmv_decreases,
  "diff_pred_pin_initial_fmv",
  "Largest FMV Decreases",
  order_scheme = "low",
  display_as_percent = TRUE
)
```

### Largest FMV (Final) Increases no Multicards

```{r, 100_largest_fmv_increases_no_multicards}}
largest_fmv_increases <- leaflet_data %>%
  group_by(meta_pin) %>%
  filter(n() == 1) %>%
  ungroup() %>%
  arrange(desc(diff_pred_pin_final_fmv)) %>%
  slice(1:100)

create_leaflet_map(
  largest_fmv_increases,
  "diff_pred_pin_final_fmv",
  "Largest FMV Increases",
  display_as_percent = TRUE
)
```

### Largest FMV (Final) Decreases no Multicards

```{r, 100_largest_fmv_decreases_no_multicards}}}
largest_fmv_decreases <- leaflet_data %>%
  group_by(meta_pin) %>%
  filter(n() == 1) %>%
  ungroup() %>%
  arrange(diff_pred_pin_initial_fmv) %>%
  slice(1:100)

create_leaflet_map(
  largest_fmv_increases,
  "diff_pred_pin_final_fmv",
  "Largest FMV Increases (%)",
  order_scheme = "low",
  display_as_percent = TRUE
)
```
:::

## Neighborhoods with the Highest and Lowest SHAP Values

These maps identify neighborhoods where the added feature is having the largest impact on SHAP values. By selecting neighborhoods with the highest mean(absolute value), you can take a closer look at how individual parcels in these neighborhoods are affected.

::: panel-tabset

```{r processing_SHAP_values}
selected_data <- leaflet_data %>%
  group_by(meta_nbhd_code) %>%
  mutate(mean_value = mean(abs(shap_total)), na.rm = TRUE) %>%
  ungroup() %>%
  distinct(meta_nbhd_code, .keep_all = TRUE) %>%
  arrange(mean_value)

# Select top 2 and bottom 2 neighborhoods based on mean SHAP values
selected_nbhd_codes <- selected_data %>%
  slice(c(1:2, (n() - 1):n())) %>%
  pull(meta_nbhd_code)


filtered_data <- filter(leaflet_data, meta_nbhd_code %in% selected_nbhd_codes)


# Separate high and low mean value neighborhoods
high_mean_data <- filtered_data %>%
  filter(
    meta_nbhd_code %in% selected_nbhd_codes[
      (length(selected_nbhd_codes) - 1):length(selected_nbhd_codes)
    ]
  )

low_mean_data <- filtered_data %>%
  filter(meta_nbhd_code %in% selected_nbhd_codes[1:2])
```

### 2 Highest SHAP Neighborhoods

```{r, 2_highest_shap_nbhd}
create_leaflet_map(high_mean_data, "shap_total", "SHAP Values")
```

### 2 Lowest SHAP Neighborhoods

```{r, 2_lowest_shap_nbhd}
create_leaflet_map(low_mean_data, "shap_total", "SHAP Values")
```

:::
